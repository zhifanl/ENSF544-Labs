{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-Recall and Receiver Operating Characteristics Curves\n",
    "Follow _Introduction to Machine Learning_ [Chapter 5](https://github.com/amueller/introduction_to_ml_with_python/blob/master/05-model-evaluation-and-improvement.ipynb)\n",
    "- **Section 5.3.2 Metrics for Binary Classification**  - Precision-recall curves and ROC curves (p.295)\n",
    "- **Section 5.3.2 Metrics for Binary Classification**  - Receiver operating characteristics (ROC) and AUC (p.299)\n",
    "\n",
    "Precision-recall curve and area under the Precsion-Recall curve aka Average precision.   \n",
    "Receiver operating characteristics curve (ROC) and area under the ROC curve aka ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_blobs(n_samples=(400, 50), cluster_std=[7.0, 2],\n",
    "                  random_state=22)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Precision-recall curve \n",
    "To investigate different thresholds and the effect on precision and recall, we can use a precision-recall curve.\n",
    "\n",
    "Note that `precision_recall_curve()` takes actual target and predicted decision function (or probability) as arguments, because it will vary the decision threshold to create the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_val, svc.decision_function(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use more data points for a smoother curve\n",
    "X, y = make_blobs(n_samples=(4000, 500), cluster_std=[7.0, 2], random_state=22)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_val, svc.decision_function(X_val))\n",
    "# find threshold closest to zero\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
    "\n",
    "plt.plot(precision, recall, label=\"precision recall curve\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the arrays produced above, use `np.argwhere()` to find threshold for a certain minimum recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_recall = 0.8\n",
    "idx = np.argwhere(recall>=desired_recall)[-1]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using probabilities of the positive class to plot precision-recall curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# RandomForestClassifier has predict_proba, but not decision_function\n",
    "precision_rf, recall_rf, thresholds_rf = precision_recall_curve(\n",
    "    y_val, rf.predict_proba(X_val)[:, 1])\n",
    "\n",
    "plt.plot(precision, recall, label=\"svc\")\n",
    "\n",
    "plt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero svc\", fillstyle=\"none\", c='k', mew=2)\n",
    "\n",
    "plt.plot(precision_rf, recall_rf, label=\"rf\")\n",
    "\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
    "plt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], '^', c='k',\n",
    "         markersize=10, label=\"threshold 0.5 rf\", fillstyle=\"none\", mew=2)\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that f1-score combines precision and recall into a single metric. This is done for the default threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"f1_score of random forest: {:.3f}\".format(\n",
    "    f1_score(y_val, rf.predict(X_val))))\n",
    "print(\"f1_score of svc: {:.3f}\".format(f1_score(y_val, svc.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Area under the Precision-Recall curve: Average precision\n",
    "The precision-recall curve explores all thresholds and contains a lot of information. One summary score is the area under the curve, also called average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_rf = average_precision_score(y_val, rf.predict_proba(X_val)[:, 1])\n",
    "ap_svc = average_precision_score(y_val, svc.decision_function(X_val))\n",
    "print(\"Average precision of random forest: {:.3f}\".format(ap_rf))\n",
    "print(\"Average precision of svc: {:.3f}\".format(ap_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Receiver operating characteristics (ROC) curve\n",
    "\n",
    "An ROC curve plots FPR (x-axis) vs. TPR (y-axis).\n",
    "\n",
    "**True positive rate (TPR):**  \n",
    "$TPR (Recall) = \\frac{TP}{TP+FN}$  \n",
    "Same as Recall. Using the *positive class* row. How many actual positive samples do we catch? This would ideally be 1.\n",
    "\n",
    "\n",
    "**False positive rate (FPR):**     \n",
    "$FPR = \\frac{FP}{FP+TN}$  \n",
    "Using the *negative class* row. Fraction of falsy identified positives. This would ideally be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Use more data points for a smoother curve\n",
    "X, y = make_blobs(n_samples=(4000, 500), cluster_std=[7.0, 2], random_state=22)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
    "\n",
    "svc = SVC(gamma=.05).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val, svc.decision_function(X_val))\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "# find threshold closest to zero\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n",
    "plt.plot([0,1], [0,1], 'k')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_val, rf.predict_proba(X_val)[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve SVC\")\n",
    "plt.plot(fpr_rf, tpr_rf, label=\"ROC Curve RF\")\n",
    "\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n",
    "         label=\"threshold zero SVC\", fillstyle=\"none\", c='k', mew=2)\n",
    "close_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\n",
    "plt.plot(fpr_rf[close_default_rf], tpr[close_default_rf], '^', markersize=10,\n",
    "         label=\"threshold 0.5 RF\", fillstyle=\"none\", c='k', mew=2)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC Area under the curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "rf_auc = roc_auc_score(y_val, rf.predict_proba(X_val)[:, 1])\n",
    "svc_auc = roc_auc_score(y_val, svc.decision_function(X_val))\n",
    "print(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\n",
    "print(\"AUC for SVC: {:.3f}\".format(svc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing a class imbalance example\n",
    "\n",
    "In the nine, not-nine classifier, there is a class imbalance: 1/10 nine, 9/10 not-nine. Hence, predicting not-nine all the time would give you an accuracy of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "y = digits.target == 9\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    digits.data, y, random_state=0)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "for gamma in [1, 0.1, 0.01]:\n",
    "    svc = SVC(gamma=gamma).fit(X_train, y_train)\n",
    "    accuracy = svc.score(X_val, y_val)\n",
    "    auc = roc_auc_score(y_val, svc.decision_function(X_val))\n",
    "    avg_prec = average_precision_score(y_val, svc.decision_function(X_val))\n",
    "    \n",
    "    print(\"gamma = {:.2f}  accuracy = {:.2f}  ROC-AUC = {:.2f} PR-AUC = {:.2f}\".format(\n",
    "          gamma, accuracy, auc, avg_prec))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_val , svc.decision_function(X_val))\n",
    "    axs[0].plot(fpr, tpr, label=\"gamma={:.3f}\".format(gamma))\n",
    "    \n",
    "    prec, rec, _ = precision_recall_curve(y_val , svc.decision_function(X_val))\n",
    "    axs[1].plot(prec, rec, label=\"gamma={:.3f}\".format(gamma))\n",
    "\n",
    "    \n",
    "axs[0].set_xlabel(\"FPR\")\n",
    "axs[0].set_ylabel(\"TPR\")\n",
    "axs[0].set_title(\"ROC\")\n",
    "axs[0].plot([0,1], [0,1], ':k')\n",
    "axs[0].set_xlim(-0.01, 1)\n",
    "axs[0].set_ylim(0, 1.02)\n",
    "axs[0].legend(loc=\"best\")\n",
    "\n",
    "axs[1].set_xlabel(\"precision\")\n",
    "axs[1].set_ylabel(\"recall\")\n",
    "axs[1].set_title(\"Precision-Recall\")\n",
    "axs[1].set_xlim(-0.01, 1.02)\n",
    "axs[1].set_ylim(0, 1.02)\n",
    "axs[1].legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like gamma=1 exactly lies on the unit diagonal. It behaves like a random guess classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The accuracy of all three settings of gamma is the same, 90%. This might be the same as chance performance, or it might not. Looking at the AUC and the corresponding curve, however, we see a clear distinction between the three models. With gamma=1.0, the AUC is actually at chance level, meaning that the output of the decision_function is as good as random. With gamma=0.1, performance drastically improves to an AUC of 0.96. Finally, with gamma=0.01, we get a perfect AUC of 1.0. That means that all positive points are ranked higher than all negative points according to the decision function. In other words, with the right threshold, this model can classify the data perfectly!5 Knowing this, we can adjust the threshold on this model and obtain great predictions. If we had only used accuracy, we would never have discovered this.\n",
    "\n",
    ">For this reason, we highly recommend using AUC when evaluating models on imbalanced data. Keep in mind that AUC does not make use of the default threshold, though, so adjusting the decision threshold might be necessary to obtain useful classification results from a model with a high AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 6. A random classifier \n",
    "The diagonal in ROC is a random classifier, let's see why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some random labels with class imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.random.choice([0, 1], 100, p=[0.7, 0.3])\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_true, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict class label randomly with 50/50 chance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.random.choice([0, 1], 100, p=[0.5, 0.5])\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_predicted, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_predicted)\n",
    "tn, fp, fn, tp =cm.ravel()\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in both rows, the predictions are split evenly. Hence, recall and false positive rate should be close to 0.5, and would lie on the diagonal. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp / (tp+fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR = fp / (fp+tn)\n",
    "FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by [this](https://datascience.stackexchange.com/questions/31872/auc-roc-of-a-random-classifier) stackexchange, if we vary the prediction probability of the positive class, we obtain different points on the ROC curve. With that we can construct the diagonal plot we anticipate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def create_random_model(y_true, n_thresholds=10, n_averages=100):\n",
    "    \"\"\"Create tpr (recall), fpr, prec variables for a random classifier\n",
    "    \n",
    "        A random model predicts:\n",
    "        - negative class with probability (1-p)\n",
    "        - postive class with probability p\n",
    "        stored in y_pred\n",
    "        \n",
    "        y_true and y_pred are used to calculate the confusion matrix,\n",
    "        and from that TPR and FPR and Precision.\n",
    "        \n",
    "        Repeated for different probabilities, aka simulated thresholds.\n",
    "    \n",
    "        y_true(np.array or list): Actual binary predictions using [0,1]\n",
    "        n_thresholds(int): Number of thresholds to simulate\n",
    "        n_averages(int): Number of repeats at each simulated threshold.\n",
    "        \n",
    "        returns list with TPRs, list with FPRs, list with Precs\n",
    "    \"\"\"\n",
    "    tprs=[]\n",
    "    fprs = []\n",
    "    precs = []\n",
    "    n_values = len(y_true)\n",
    "    for p in np.linspace(0.1,0.9,n_thresholds):\n",
    "        tpr=0\n",
    "        fpr=0\n",
    "        prec=0\n",
    "        # to obtain smoother curves, we repeat the random model \n",
    "        # at each threshold and average.\n",
    "        # an alternative would be to increase the length of y_true\n",
    "        for i in range(n_averages):\n",
    "            y_pred = np.random.choice([0, 1], n_values, p=[1-p, p])\n",
    "            mat = confusion_matrix(y_true, y_pred) \n",
    "            tn, fp, fn, tp = mat.ravel()\n",
    "            tpr += tp/(tp+fn)\n",
    "            fpr += fp/(fp+tn)\n",
    "            prec += tp / (tp+fp)\n",
    "        tprs.append(tpr/n_averages)\n",
    "        fprs.append(fpr/n_averages)\n",
    "        precs.append(prec/n_averages)\n",
    "        \n",
    "    return tprs, fprs, precs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "y_true = np.random.choice([0, 1], 100, p=[0.7, 0.3])\n",
    "tprs, fprs, precs= create_random_model(y_true)\n",
    "\n",
    "axs[0].plot(fprs, tprs)\n",
    "axs[0].grid(True)\n",
    "axs[0].set_xlabel('FPR')\n",
    "axs[0].set_ylabel('TPR')\n",
    "axs[0].set_title('ROC of a random classifier');\n",
    "\n",
    "axs[1].plot(precs, tprs)\n",
    "axs[1].set_xlim([0,1])\n",
    "axs[1].grid(True)\n",
    "axs[1].set_xlabel('Precision')\n",
    "axs[1].set_ylabel('Recall')\n",
    "axs[1].set_title('Precision-Recall of a random classifier');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a random classifier is the diagonal in an ROC plot, a random classifier is a vertical line in the precision-recall plot. Precision is constant and equal to the probability of positive class (here between 0.2 and 0.3). Recall is equal to the prediction probabilty of the random classifier, which is varied between 0.1 and 0.9 here.\n",
    "\n",
    "See here for the math: https://stats.stackexchange.com/questions/89495/precision-and-recall-of-a-random-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
