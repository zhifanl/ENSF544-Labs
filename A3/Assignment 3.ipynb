{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (38 total marks)\n",
    "### Due: March 5 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "from yellowbrick import datasets\n",
    "\n",
    "X, y = datasets.load_concrete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "47f7db47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-21 {color: black;}#sk-container-id-21 pre{padding: 0;}#sk-container-id-21 div.sk-toggleable {background-color: white;}#sk-container-id-21 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-21 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-21 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-21 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-21 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-21 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-21 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-21 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-21 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-21 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-21 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-21 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-21 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-21 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-21 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-21 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-21 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-21 div.sk-item {position: relative;z-index: 1;}#sk-container-id-21 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-21 div.sk-item::before, #sk-container-id-21 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-21 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-21 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-21 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-21 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-21 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-21 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-21 div.sk-label-container {text-align: center;}#sk-container-id-21 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-21 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-21\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(max_depth=5, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" checked><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=5, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "rf = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "gb = GradientBoostingRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "dt.fit(X,y)\n",
    "\n",
    "rf.fit(X,y)\n",
    "\n",
    "gb.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f2969643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Regressor - Train MSE: 47.91856102734339, Validation MSE: 163.08777547307804\n",
      "RF Regressor - Train MSE: 32.056464386029816, Validation MSE: 156.251425313443\n",
      "GB Regressor - Train MSE: 3.7392700109420987, Validation MSE: 99.2245764199326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def get_accuracy(model, X, y, scoring):\n",
    "    cv_results = cross_validate(model, X, y, cv=5, scoring=scoring, return_train_score=True)\n",
    "    train_mse = -1 * cv_results['train_score'].mean()\n",
    "    validation_mse = -1 * cv_results['test_score'].mean()\n",
    "    return train_mse, validation_mse\n",
    "\n",
    "dt_train_mse, dt_validation_mse = get_accuracy(dt, X, y, 'neg_mean_squared_error')\n",
    "print(f\"DT Regressor - Train MSE: {dt_train_mse}, Validation MSE: {dt_validation_mse}\")\n",
    "\n",
    "rf_train_mse, rf_validation_mse = get_accuracy(rf, X, y, 'neg_mean_squared_error')\n",
    "print(f\"RF Regressor - Train MSE: {rf_train_mse}, Validation MSE: {rf_validation_mse}\")\n",
    "\n",
    "gb_train_mse, gb_validation_mse = get_accuracy(gb, X, y, 'neg_mean_squared_error')\n",
    "print(f\"GB Regressor - Train MSE: {gb_train_mse}, Validation MSE: {gb_validation_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fdc93a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model  Training Accuracy  Validation Accuracy\n",
      "0    DT          47.918561           163.087775\n",
      "1    RF          32.056464           156.251425\n",
      "2    GB           3.739270            99.224576\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Training Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "results.loc[len(results)]= {\"Model\": 'DT', \"Training Accuracy\": dt_train_mse, \"Validation Accuracy\": dt_validation_mse}\n",
    "results.loc[len(results)]= {\"Model\": 'RF', \"Training Accuracy\": rf_train_mse, \"Validation Accuracy\": rf_validation_mse}\n",
    "results.loc[len(results)]= {\"Model\": 'GB', \"Training Accuracy\": gb_train_mse, \"Validation Accuracy\": gb_validation_mse}\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4434bb",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "843c90dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Regressor - Train r2: -0.8228872809524459, Validation r2: -0.1762104452178903\n",
      "RF Regressor - Train r2: -0.8812176248882352, Validation r2: -0.17478141576183334\n",
      "GB Regressor - Train r2: -0.9864362663137645, Validation r2: -0.47442497151979585\n",
      "  Model  Training R2  Validation R2\n",
      "0    DT    -0.822887      -0.176210\n",
      "1    RF    -0.881218      -0.174781\n",
      "2    GB    -0.986436      -0.474425\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "dt_train_r2, dt_validation_r2 = get_accuracy(dt, X, y, 'r2')\n",
    "print(f\"DT Regressor - Train r2: {dt_train_r2}, Validation r2: {dt_validation_r2}\")\n",
    "\n",
    "rf_train_r2, rf_validation_r2 = get_accuracy(rf, X, y, 'r2')\n",
    "print(f\"RF Regressor - Train r2: {rf_train_r2}, Validation r2: {rf_validation_r2}\")\n",
    "\n",
    "gb_train_r2, gb_validation_r2 = get_accuracy(gb, X, y, 'r2')\n",
    "print(f\"GB Regressor - Train r2: {gb_train_r2}, Validation r2: {gb_validation_r2}\")\n",
    "\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Training R2\", \"Validation R2\"])\n",
    "\n",
    "results.loc[len(results)]= {\"Model\": 'DT', \"Training R2\": dt_train_r2, \"Validation R2\": dt_validation_r2}\n",
    "results.loc[len(results)]= {\"Model\": 'RF', \"Training R2\": rf_train_r2, \"Validation R2\": rf_validation_r2}\n",
    "results.loc[len(results)]= {\"Model\": 'GB', \"Training R2\": gb_train_r2, \"Validation R2\": gb_validation_r2}\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "1. Out of the models you tested, which model would you select for this dataset and why?\n",
    "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. The accuracy of training is high but the validation accuracy is way lower than assignment 2's.The training score is close to 1.0, but the validation score is much lower, the model isoverfitting. Assignment 2 has both high training and validation score.\n",
    "2. I will select the LogisticRegression Model with X and y as training set. Because that model has both decent high training score and testing score among these logistic regression models. And since the tree models' max depthes are all only 5, and validation scores are low with max depth = 5. This means the models are already overfitting at the start of increasing more complexity. I would not choose tree models in this case. \n",
    "3. Hyperparameter Tuning: Increase & Decrease the max_depth parameter to prevent overfitting.\n",
    "Feature Engineering: Drop the unnecessary columns and retain only those features that contribute most to target variable. This reduces model complexity and improves interpretability of the model make the accuracy increase and reduce the noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9ccc3",
   "metadata": {},
   "source": [
    "1. Where did you source your code?\n",
    "- Based on lecture materials and labs and assignment 2.\n",
    "2. In what order did you complete the steps?\n",
    "- Try to come up with solutions by myself. If I see challenges. Ex: cannot memorize a function's parameters, or don't know which method to use, I will google or go to lecture code examples.\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "- I ask specific questions, not copy and paste the content to the AI. Yes I need to modify the code to help me memorize what it does. then next time I don't need to ask ai. Specifically I let AI to help me with some ideas with making the tree models perform better, I found the idea I wanted and also some new ideas that I didn't learn in the lecture. It was a good tool to extend my learning and help me with the reasoning though I came up with the answer by myself in my own way of thinking without using AI.\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "- I had challenges with not understanding what is the cause of low validation score and high training score so I did some research and found out it is because of overfitting. I also had challenges with coming up with ideas that help improve the performance of these tree models. I looked at the lecture material and that helped me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (18.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 109, 'name': 'Wine', 'repository_url': 'https://archive.ics.uci.edu/dataset/109/wine', 'data_url': 'https://archive.ics.uci.edu/static/public/109/data.csv', 'abstract': 'Using chemical analysis to determine the origin of wines', 'area': 'Physics and Chemistry', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 178, 'num_features': 13, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1992, 'last_updated': 'Mon Aug 28 2023', 'dataset_doi': '10.24432/C5PC7J', 'creators': ['Stefan Aeberhard', 'M. Forina'], 'intro_paper': {'title': 'Comparative analysis of statistical pattern recognition methods in high dimensional settings', 'authors': 'S. Aeberhard, D. Coomans, O. Vel', 'published_in': 'Pattern Recognition', 'year': 1994, 'url': 'https://www.semanticscholar.org/paper/83dc3e4030d7b9fbdbb4bde03ce12ab70ca10528', 'doi': '10.1016/0031-3203(94)90145-7'}, 'additional_info': {'summary': 'These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. \\r\\n\\r\\nI think that the initial data set had around 30 variables, but for some reason I only have the 13 dimensional version. I had a list of what the 30 or so variables were, but a.)  I lost it, and b.), I would not know which 13 variables are included in the set.\\r\\n\\r\\nThe attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it )\\r\\n1) Alcohol\\r\\n2) Malic acid\\r\\n3) Ash\\r\\n4) Alcalinity of ash  \\r\\n5) Magnesium\\r\\n6) Total phenols\\r\\n7) Flavanoids\\r\\n8) Nonflavanoid phenols\\r\\n9) Proanthocyanins\\r\\n10)Color intensity\\r\\n11)Hue\\r\\n12)OD280/OD315 of diluted wines\\r\\n13)Proline \\r\\n\\r\\nIn a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging.           ', 'purpose': 'test', 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'All attributes are continuous\\r\\n\\t\\r\\nNo statistics available, but suggest to standardise variables for certain uses (e.g. for us with classifiers which are NOT scale invariant)\\r\\n\\r\\nNOTE: 1st attribute is class identifier (1-3)', 'citation': None}}\n",
      "                            name     role         type demographic  \\\n",
      "0                          class   Target  Categorical        None   \n",
      "1                        Alcohol  Feature   Continuous        None   \n",
      "2                      Malicacid  Feature   Continuous        None   \n",
      "3                            Ash  Feature   Continuous        None   \n",
      "4              Alcalinity_of_ash  Feature   Continuous        None   \n",
      "5                      Magnesium  Feature      Integer        None   \n",
      "6                  Total_phenols  Feature   Continuous        None   \n",
      "7                     Flavanoids  Feature   Continuous        None   \n",
      "8           Nonflavanoid_phenols  Feature   Continuous        None   \n",
      "9                Proanthocyanins  Feature   Continuous        None   \n",
      "10               Color_intensity  Feature   Continuous        None   \n",
      "11                           Hue  Feature   Continuous        None   \n",
      "12  0D280_0D315_of_diluted_wines  Feature   Continuous        None   \n",
      "13                       Proline  Feature      Integer        None   \n",
      "\n",
      "   description units missing_values  \n",
      "0         None  None             no  \n",
      "1         None  None             no  \n",
      "2         None  None             no  \n",
      "3         None  None             no  \n",
      "4         None  None             no  \n",
      "5         None  None             no  \n",
      "6         None  None             no  \n",
      "7         None  None             no  \n",
      "8         None  None             no  \n",
      "9         None  None             no  \n",
      "10        None  None             no  \n",
      "11        None  None             no  \n",
      "12        None  None             no  \n",
      "13        None  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "wine = fetch_ucirepo(id=109) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = wine.data.features \n",
    "y = wine.data.targets \n",
    "\n",
    "# metadata \n",
    "print(wine.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(wine.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908942db",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "033dded2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n",
      "0    14.23       1.71  2.43               15.6        127           2.80   \n",
      "1    13.20       1.78  2.14               11.2        100           2.65   \n",
      "2    13.16       2.36  2.67               18.6        101           2.80   \n",
      "3    14.37       1.95  2.50               16.8        113           3.85   \n",
      "4    13.24       2.59  2.87               21.0        118           2.80   \n",
      "\n",
      "   Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n",
      "0        3.06                  0.28             2.29             5.64  1.04   \n",
      "1        2.76                  0.26             1.28             4.38  1.05   \n",
      "2        3.24                  0.30             2.81             5.68  1.03   \n",
      "3        3.49                  0.24             2.18             7.80  0.86   \n",
      "4        2.69                  0.39             1.82             4.32  1.04   \n",
      "\n",
      "   0D280_0D315_of_diluted_wines  Proline  \n",
      "0                          3.92     1065  \n",
      "1                          3.40     1050  \n",
      "2                          3.17     1185  \n",
      "3                          3.45     1480  \n",
      "4                          2.93      735  \n",
      "   class\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(X.head())\n",
    "\n",
    "\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a3c2f",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "88413313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol                         0\n",
      "Malicacid                       0\n",
      "Ash                             0\n",
      "Alcalinity_of_ash               0\n",
      "Magnesium                       0\n",
      "Total_phenols                   0\n",
      "Flavanoids                      0\n",
      "Nonflavanoid_phenols            0\n",
      "Proanthocyanins                 0\n",
      "Color_intensity                 0\n",
      "Hue                             0\n",
      "0D280_0D315_of_diluted_wines    0\n",
      "Proline                         0\n",
      "dtype: int64\n",
      "class    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(X.isnull().sum())\n",
    "\n",
    "print(y.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db81660",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8f38fdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "2        71\n",
       "1        59\n",
       "3        48\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "wine_counts = y.value_counts()\n",
    "wine_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `RandomForestClassifier(max_depth = 2)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3182d5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-22 {color: black;}#sk-container-id-22 pre{padding: 0;}#sk-container-id-22 div.sk-toggleable {background-color: white;}#sk-container-id-22 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-22 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-22 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-22 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-22 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-22 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-22 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-22 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-22 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-22 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-22 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-22 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-22 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-22 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-22 div.sk-item {position: relative;z-index: 1;}#sk-container-id-22 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-22 div.sk-item::before, #sk-container-id-22 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-22 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-22 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-22 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-22 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-22 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-22 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-22 div.sk-label-container {text-align: center;}#sk-container-id-22 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-22 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-22\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=2, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" checked><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=2, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "svc = SVC(random_state = 0)\n",
    "rf = RandomForestClassifier(max_depth = 2, random_state = 0)\n",
    "\n",
    "y = y.values.ravel()\n",
    "\n",
    "svc.fit(X, y)\n",
    "\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "424bac48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc Regressor - Train accuracy: 0.7037427361371023, Validation accuracy: 0.6634920634920635\n",
      "RF Regressor - Train accuracy: 0.9859647394858662, Validation accuracy: 0.972063492063492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def get_accuracy(model, X, y, scoring):\n",
    "    cv_results = cross_validate(model, X, y, cv=5, scoring=scoring, return_train_score=True)\n",
    "    train_accuracy =  cv_results['train_score'].mean()\n",
    "    validation_accuracy = cv_results['test_score'].mean()\n",
    "    return train_accuracy, validation_accuracy\n",
    "\n",
    "svc_train_accuracy, svc_validation_accuracy = get_accuracy(svc, X, y, 'accuracy')\n",
    "print(f\"svc Regressor - Train accuracy: {svc_train_accuracy}, Validation accuracy: {svc_validation_accuracy}\")\n",
    "\n",
    "rf_train_accuracy, rf_validation_accuracy = get_accuracy(rf, X, y, 'accuracy')\n",
    "print(f\"RF Regressor - Train accuracy: {rf_train_accuracy}, Validation accuracy: {rf_validation_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model  Training Accuracy  Validation Accuracy\n",
      "0   SVC           0.703743             0.663492\n",
      "1    RF           0.985965             0.972063\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5.1\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Training Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "results.loc[len(results)]= {\"Model\": 'SVC', \"Training Accuracy\": svc_train_accuracy, \"Validation Accuracy\": svc_validation_accuracy}\n",
    "results.loc[len(results)]= {\"Model\": 'RF', \"Training Accuracy\": rf_train_accuracy, \"Validation Accuracy\": rf_validation_accuracy}\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8e832",
   "metadata": {},
   "source": [
    "#### Step 5.2: Improve Model Performance\n",
    "\n",
    "As stated in class, support vector machines require additional pre-processing compared to tree-based models. Write the code below to test three different scaling methods, `MinMaxScaler`, `StandardScaler` and `RobustScaler`. For this case, use the same cross-validation method mentioned in the previous step. Print the training and validation accuracy results in a table. Use the default parameters for the `SVC`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "283a379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Test different scaling methods for the SVM model    \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "X_scaled_minmax = minmax_scaler.fit_transform(X)\n",
    "X_scaled_standard = standard_scaler.fit_transform(X)\n",
    "X_scaled_robust = robust_scaler.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a17bc58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC - Train Accuracy with MinMax Scaler: 0.9957943464985718, Validation Accuracy: 0.9776190476190477\n",
      "SVC - Train Accuracy with Standard Scaler: 0.9986013986013986, Validation Accuracy: 0.9833333333333334\n",
      "SVC - Train Accuracy with Robust Scaler: 0.9986013986013986, Validation Accuracy: 0.9722222222222221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def get_accuracy(svc, X, y, scoring):\n",
    "    svc.fit(X, y)\n",
    "    cv_results = cross_validate(svc, X, y, cv=5, scoring=scoring, return_train_score=True)\n",
    "    train_accuracy = cv_results['train_score'].mean()\n",
    "    validation_accuracy = cv_results['test_score'].mean()\n",
    "    return train_accuracy, validation_accuracy\n",
    "\n",
    "svc = SVC(random_state = 0)\n",
    "\n",
    "svc_train_accuracy_minmax, svc_validation_accuracy_minmax = get_accuracy(svc, X_scaled_minmax, y, 'accuracy')\n",
    "print(f\"SVC - Train Accuracy with MinMax Scaler: {svc_train_accuracy_minmax}, Validation Accuracy: {svc_validation_accuracy_minmax}\")\n",
    "\n",
    "svc_train_accuracy_standard, svc_validation_accuracy_standard = get_accuracy(svc, X_scaled_standard, y, 'accuracy')\n",
    "print(f\"SVC - Train Accuracy with Standard Scaler: {svc_train_accuracy_standard}, Validation Accuracy: {svc_validation_accuracy_standard}\")\n",
    "\n",
    "svc_train_accuracy_robust, svc_validation_accuracy_robust = get_accuracy(svc, X_scaled_robust, y, 'accuracy')\n",
    "print(f\"SVC - Train Accuracy with Robust Scaler: {svc_train_accuracy_robust}, Validation Accuracy: {svc_validation_accuracy_robust}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (7 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used (without scaling)? Were either of these models a good fit for the data?\n",
    "1. What are two reasons why a support vector machines model might not work as well as a tree-based model?\n",
    "1. How did each scaler perform compared to the unscaled results? Was there a significant difference in the performance of the scalers comparatively? Explain with values.\n",
    "1. How did the results for the scaled SVM model compare to the random forest model? \n",
    "\n",
    "*YOUR ANSWERS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f7567",
   "metadata": {},
   "source": [
    "1. \n",
    "\n",
    "svc: - Train accuracy: 0.7037427361371023, Validation accuracy: 0.6634920634920635\n",
    "\n",
    "Random Forest - Train accuracy: 0.9859647394858662, Validation accuracy: 0.972063492063492\n",
    "\n",
    "SVC is not a good fit for the data because both training and validation accuracy are low. The model is underfit. it might not capture the complexity of the data as effectively as the Random Forest model.\n",
    "\n",
    "Random Forest is a good fit for the data since both training and validating accuracy are very close to 1.\n",
    "\n",
    "2. \n",
    "Reason 1: SVC are sensitive to the scale of the features, this means that if the features are not on the same scale, the SVM might prioritize the features with larger scales, thus the model won't work well. Tree-based models like Random Forests, on the other hand, are less sensitive to feature scaling since they make decisions based on splits and do not rely on the magnitude of the feature values. This is why it doesn't win over Random Forest model\n",
    "Reason 2: SVC struggles with datasets that have complex, non-linear relationships. Kernel (like RBF) can help SVMs capture non-linear data and trend in the model, tree-based models naturally is good at handling non-linear data by dividing the feature space into simpler, piece-wise segments. \n",
    "\n",
    "3. \n",
    "There is no significant difference on accuracies by appling these different scalers. scaling can significantly impact the performance of models, especially for SVM that are sensitive to the scale of input features. Scalers MinMaxScaler, StandardScaler, or RobustScaler can bring all features to a similar scale, which makes the data centered at the origin. (x=0, y=0) that's why the accuracy with these scaled data are similarily high.\n",
    "\n",
    "SVC - Train Accuracy with MinMax Scaler: 0.9957943464985718, Validation Accuracy: 0.9776190476190477\n",
    "SVC - Train Accuracy with Standard Scaler: 0.9986013986013986, Validation Accuracy: 0.9833333333333334\n",
    "SVC - Train Accuracy with Robust Scaler: 0.9986013986013986, Validation Accuracy: 0.9722222222222221\n",
    "\n",
    "4. \n",
    "The performance of SVM with scaling has similar high performance like the Random Forest model:\n",
    "\n",
    "SVC - Train Accuracy with MinMax Scaler: 0.9957943464985718, Validation Accuracy: 0.9776190476190477\n",
    "SVC - Train Accuracy with Standard Scaler: 0.9986013986013986, Validation Accuracy: 0.9833333333333334\n",
    "SVC - Train Accuracy with Robust Scaler: 0.9986013986013986, Validation Accuracy: 0.9722222222222221\n",
    "Random Forest     Train Accuracy:      0.985965      Validation Accuracy:       0.972063"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*\n",
    "1. Where did you source your code?\n",
    "- Based on lecture materials from Week6 and Question 1 from A3.\n",
    "2. In what order did you complete the steps?\n",
    "- Try to come up with solutions by myself. If I see challenges. Ex: cannot give the reason why random forest has better performance than svm without scaling. I will google or go to lecture code examples.\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "- I ask specific questions, not copy and paste the content to the AI. Yes I need to modify the code to help me memorize what it does. then next time I don't need to ask ai. Specifically I let AI to help me with some ideas why scaling can make the SVM perform better, I found the idea I wanted and also some new ideas that I didn't learn in the lecture. It was a good tool to extend my learning and help me with the reasoning though I came up with the answer by myself in my own way of thinking without using AI.\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "- I had challenges with not understanding what is the cause of low validation accuracy and low training accuracy, so I did some research and found out it is because of underfitting. I also had challenges with coming up with ideas that help improve the performance of SVM. I looked at the lecture material and that helped me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "\n",
    "I found different scaler won't make a huge different in this dataset with these features. The reason is they are scaled into similar kind of data, not like norm scalers. \n",
    "\n",
    "I also found sometimes when you are not sure if should use a scaler or not, you can always use random forest since it splits the data into branches instead of looking at its magnitude.\n",
    "\n",
    "I noticed that sometimes if accuracy of training and validation is low, it doesn't mean the model is not good fit. it sometimes has something to do with its features, we need to do preprocessing and feature engineering to make the model perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf91320",
   "metadata": {},
   "source": [
    "Working on this assignment, I found the process of scaling and how scaling impact SVM model performance, and explanation why RF doesn't need scaling because the feature is not used as magnitude but splitted into decision branches. I learned a lot and memorized lots of concepts by doing this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
